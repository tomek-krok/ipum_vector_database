{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. connect to the database. Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.12.3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    host=host,\n",
    "    port=port\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vector databases work quite similarly to document databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "VECTOR_LENGTH = 768  # check the dimensionality for Silver Retriever Base (v1.1) model\n",
    "\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"Primary id\")\n",
    "text = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=4096, description=\"Page text\")\n",
    "embedding_text = FieldSchema(\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_LENGTH, description=\"Embedded text\")\n",
    "\n",
    "fields = [id_field, text, embedding_text]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, auto_id=True, enable_dynamic_field=True, description=\"RAG Texts collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. To create a collection with the given schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection rag_texts_and_embeddings exists: True True\n",
      "['rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 457850165906902359, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 457850344056029189}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "has_collection = milvus_client.has_collection(COLLECTION_NAME)\n",
    "\n",
    "print(f\"Collection {COLLECTION_NAME} exists: {has_collection}\", has_collection)\n",
    "\n",
    "if not has_collection:\n",
    "    milvus_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        schema=schema\n",
    "    )\n",
    "    \n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\", \n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64}  # lower values for speed\n",
    ") \n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now we are able to insert documents into put database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data source and destination\n",
    "## the document origin destination from which document will be downloaded \n",
    "pdf_url = \"https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the document\n",
    "file_name = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the processed document \n",
    "file_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.json\"\n",
    "\n",
    "## local destination of the embedded pages of the document\n",
    "embeddings_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska-Embeddings.json\"\n",
    "\n",
    "## local destination of all above local required files\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Let's download the document into the `data_dir` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_pdf_data(pdf_url: str, file_name: str) -> None:\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as file:\n",
    "        for block in response.iter_content(chunk_size=1024):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "\n",
    "download_pdf_data(pdf_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. This is a lot of text, and in RAG we need to add specific fragments to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "import fitz\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_name, file_json):\n",
    "    document = fitz.open(os.path.join(data_dir, file_name))\n",
    "    pages = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "\n",
    "    with open(os.path.join(data_dir, file_json), \"w\") as file:\n",
    "        json.dump(pages, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "extract_pdf_text(file_name, file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now we have texts, but we need vectors. We will use the model to embed text from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def generate_embeddings(file_json, embeddings_json, model):\n",
    "    pages = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for page in data:\n",
    "        pages.append(page[\"text\"])\n",
    "\n",
    "    embeddings = model.encode(pages)\n",
    "\n",
    "    embeddings_paginated = []\n",
    "    for page_num in range(len(embeddings)):\n",
    "        embeddings_paginated.append({\"page_num\": page_num, \"embedding\": embeddings[page_num].tolist()})\n",
    "\n",
    "    with open(os.path.join(data_dir, embeddings_json), \"w\") as file:\n",
    "        json.dump(embeddings_paginated, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "model_name = \"ipipan/silver-retriever-base-v1.1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "generate_embeddings(file_json, embeddings_json, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Now we can easily insert the data into Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings(file_json, embeddings_json, client=milvus_client):\n",
    "    rows = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as t_f, open(os.path.join(data_dir, embeddings_json), \"r\") as e_f:\n",
    "        text_data, embedding_data = json.load(t_f), json.load(e_f)\n",
    "        text_data =  list(map(lambda d: d[\"text\"], text_data))\n",
    "        embedding_data = list(map(lambda d: d[\"embedding\"], embedding_data))\n",
    "        \n",
    "        for page, (text, embedding) in enumerate(zip(text_data, embedding_data)):\n",
    "            rows.append({\"text\":text, \"embedding\": embedding})\n",
    "\n",
    "    client.insert(collection_name=\"rag_texts_and_embeddings\", data=rows)\n",
    "\n",
    "\n",
    "insert_embeddings(file_json, embeddings_json)\n",
    "\n",
    "# load inserted data into memory\n",
    "milvus_client.load_collection(\"rag_texts_and_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Now let's do some semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia powstania\n",
      "sztucznej inteligencji\n",
      "7\n",
      "W języku potocznym „sztuczny\" oznacza to, co\n",
      "jest \n",
      "wytworem \n",
      "mającym \n",
      "naśladować \n",
      "coś\n",
      "naturalnego. W takim znaczeniu używamy\n",
      "terminu ,,sztuczny'', gdy mówimy o sztucznym\n",
      "lodowisku lub oku. Sztuczna inteligencja byłaby\n",
      "czymś (programem, maszyną) symulującym\n",
      "inteligencję naturalną, ludzką.\n",
      "Sztuczna inteligencja (AI) to obszar informatyki,\n",
      "który skupia się na tworzeniu programów\n",
      "komputerowych zdolnych do wykonywania\n",
      "zadań, które wymagają ludzkiej inteligencji. \n",
      "Te zadania obejmują rozpoznawanie wzorców,\n",
      "rozumienie języka naturalnego, podejmowanie\n",
      "decyzji, uczenie się, planowanie i wiele innych.\n",
      "Głównym celem AI jest stworzenie systemów,\n",
      "które są zdolne do myślenia i podejmowania\n",
      "decyzji na sposób przypominający ludzki.\n",
      "Historia sztucznej inteligencji sięga lat 50. \n",
      "XX wieku, kiedy to powstały pierwsze koncepcje\n",
      "i modele tego, co mogłoby stać się sztuczną\n",
      "inteligencją. Jednym z pionierów był Alan\n",
      "Turing, który sformułował test Turinga, mający\n",
      "na \n",
      "celu \n",
      "ocenę \n",
      "zdolności \n",
      "maszyny \n",
      "do\n",
      "inteligentnego \n",
      "zachowania \n",
      "na \n",
      "poziomie\n",
      "ludzkim. Jednakże dopiero w latach 80. i 90.\n",
      "nastąpił \n",
      "prawdziwy \n",
      "przełom \n",
      "w \n",
      "dziedzinie\n",
      "sztucznej \n",
      "inteligencji \n",
      "dzięki \n",
      "postępowi \n",
      "w\n",
      "dziedzinie algorytmów uczenia maszynowego.\n",
      "W wypadku sztucznej inteligencji mamy na\n",
      "uwadze system, który realizowałby niektóre\n",
      "funkcje \n",
      "umysłu \n",
      "– \n",
      "czasami \n",
      "w \n",
      "sposób\n",
      "przewyższający funkcje naturalne (na przykład,\n",
      "aby był wolny od pomyłek przy liczeniu oraz\n",
      "defektów \n",
      "pamięci). \n",
      "Inteligencja \n",
      "jest \n",
      "wła-\n",
      "ściwością umysłu. \n",
      "Składa się na nią szereg umiejętności, takich jak\n",
      "zdolność do komunikowania, rozwiązywania\n",
      "problemów, uczenia się i dostosowywania do\n",
      "sytuacji. \n",
      "Istotna \n",
      "jest \n",
      "jednak \n",
      "umiejętność\n",
      "rozumowania.\n",
      "Współczesne systemy sztucznej inteligencji są\n",
      "inteligentne tylko w ograniczonym obszarze. \n",
      "Na przykład komputer potrafi grać w szachy w\n",
      "taki \n",
      "sposób, \n",
      "że \n",
      "wygrywa \n",
      "z \n",
      "szachowym\n",
      "arcymistrzem. W 1996 r. Deep Blue wygrał jedną\n",
      "partię \n",
      "szachów \n",
      "z \n",
      "Garry \n",
      "Kasparowem,\n",
      "przegrywając cały mecz wynikiem 4:2 (przy\n",
      "dwóch remisach).\n",
      "Później Deep Blue został ulepszony i nie-\n",
      "oficjalnie \n",
      "nazwany \n",
      "„Deeper \n",
      "Blue\". \n",
      "Zagrał\n",
      "ponownie z Kasparowem w maju 1997 roku.\n",
      "Mecz \n",
      "skończył \n",
      "się \n",
      "wynikiem \n",
      "3½:2½ \n",
      "dla\n",
      "komputera. W ten sposób Deep Blue stał się\n",
      "pierwszym systemem komputerowym, który\n",
      "wygrał z aktualnym mistrzem świata w meczu\n",
      "ze standardową kontrolą czasu.\n",
      "Źródło: Midjourney – obraz wygenerowany przez AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# search\n",
    "def search(model, query, client=milvus_client):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    result = client.search(\n",
    "        collection_name=\"rag_texts_and_embeddings\", \n",
    "        data=[embedded_query], \n",
    "        limit=1,\n",
    "        search_params={\"metric_type\": \"L2\"},\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "result = search(model, query=\"Czym jest sztuczna inteligencja\")\n",
    "\n",
    "print(result[0][0][\"entity\"][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Let's prepare the function that will call Google API and generate our response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "# GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_KEY =\"AIzaSyAfdnCac5xkRzBZQyzkWECq-nds75b-6EQ\"\n",
    "gemini_client = genai.Client(api_key=GEMINI_KEY)\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    try:\n",
    "        # Send request to Gemini 2.0 Flash API and get the response\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now we can fully integrate everything into a RAG system. Fill the function below that will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context: str, query: str) -> str:\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    return prompt\n",
    "    \n",
    "\n",
    "def rag(query: str) -> str:\n",
    "    result = search(model, query)\n",
    "    context = result[0][0][\"entity\"][\"text\"]\n",
    "    prompt = build_prompt(context, query)\n",
    "    response = generate_response(prompt)\n",
    "    return response\n",
    "    # having all prepared functions, you can combine them together and try to build your own RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# response formating to print short lines in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Sztuczna inteligencja (AI) to obszar informatyki, który skupia się na tworzeniu programów\n",
      "komputerowych zdolnych do wykonywania zadań, które wymagają ludzkiej inteligencji. Te zadania\n",
      "obejmują rozpoznawanie wzorców, rozumienie języka naturalnego, podejmowanie decyzji, uczenie się,\n",
      "planowanie i wiele innych. Głównym celem AI jest stworzenie systemów, które są zdolne do myślenia i\n",
      "podejmowania decyzji na sposób przypominający ludzki. W potocznym rozumieniu, sztuczna inteligencja\n",
      "to coś (program, maszyna) symulujące inteligencję naturalną, ludzką.\n"
     ]
    }
   ],
   "source": [
    "query=\"Czym jest sztuczna inteligencja\"\n",
    "\n",
    "response =rag(query)\n",
    "\n",
    "print(type(response))\n",
    "\n",
    "\n",
    "wrapped_response = \"\\n\".join(textwrap.wrap(response, width=100))\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z tekstu wynika, że rozwój sztucznej inteligencji zmierza w kilku kierunkach:  *   **W kierunku\n",
      "coraz większego zrozumienia i emulacji ludzkiej inteligencji:** Od wąskiej AI, przez silną AI, aż po\n",
      "umysłową AI, która ma emulować ludzkie myślenie i rozumienie, a nawet posiadać świadomość. *   **W\n",
      "kierunku współpracy i synergii z ludzką inteligencją:** Sztuczna inteligencja zwiększająca\n",
      "(Augmented Intelligence) pokazuje trend łączenia sił AI z ludzkimi zdolnościami. *   **W kierunku\n",
      "bezpieczeństwa i etyki:** Sztuczna inteligencja bezpieczna (Safe AI) pokazuje, że istotne jest\n",
      "minimalizowanie ryzyka i zapewnienie zgodności z wartościami etycznymi. *   **W kierunku lepszego\n",
      "zrozumienia ludzkich procesów myślowych:** Sztuczna inteligencja odwrócona (Inverse AI) dąży do\n",
      "modelowania ludzkiego myślenia, by lepiej dostosowywać interakcje maszyn do ludzkich oczekiwań. *\n",
      "**W kierunku reprezentacji wiedzy o świecie i przewidywania przyszłości:** Sztuczna inteligencja\n",
      "reprezentacyjna (Representational AI) i predykcyjna (Predictive AI) skupiają się na tworzeniu modeli\n",
      "wiedzy i przewidywaniu wyników.  Podsumowując, rozwój AI ma na celu nie tylko tworzenie coraz\n",
      "bardziej zaawansowanych i autonomicznych systemów, ale również zapewnienie ich bezpieczeństwa,\n",
      "etycznego działania oraz harmonijnej współpracy z ludźmi.\n"
     ]
    }
   ],
   "source": [
    "query=\"Jaki jest kierunek rozwoju sztucznej inteligencji\"\n",
    "\n",
    "wrapped_response = \"\\n\".join(textwrap.wrap(rag(query), width=100))\n",
    "print(wrapped_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
